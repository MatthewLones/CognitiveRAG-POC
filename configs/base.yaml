# Base configuration for RAG system
models:
  embedding_model: "all-MiniLM-L6-v2"
  llm_model: "gpt-4-turbo-preview"
  reranker_model: "cross-encoder/ms-marco-MiniLM-L-6-v2"

# Chunking parameters
chunking:
  chunk_size: 600
  chunk_overlap: 80
  separators: ["\n\n", "\n", ".", "!", "?", ";", " "]

# Retrieval parameters
retrieval:
  top_k: 10
  rerank_top_k: 5
  fusion_method: "rrf"  # reciprocal rank fusion
  bm25_weight: 0.3
  dense_weight: 0.7

# Prompt fanning
prompt_fanning:
  enabled: true
  max_subqueries: 3
  subquery_types: ["definition", "examples", "latest_info"]

# Guardrails
guardrails:
  answerability_threshold: 0.7
  self_check_enabled: true
  max_citations: 5

# Performance
performance:
  max_tokens: 4000  # GPT-4 turbo limit is 4096
  temperature: 0.1
  timeout: 30
